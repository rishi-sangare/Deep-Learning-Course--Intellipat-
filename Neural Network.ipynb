{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04afa3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf8749b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of black people by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "data=load_boston(return_X_y=False)\n",
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76eb64ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(r\"boston_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9a3328b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0    0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1    0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2    0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3    0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4    0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n",
       "501  0.06263   0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n",
       "502  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n",
       "503  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n",
       "504  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n",
       "505  0.04741   0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  MEDV  \n",
       "0       15.3  396.90   4.98  24.0  \n",
       "1       17.8  396.90   9.14  21.6  \n",
       "2       17.8  392.83   4.03  34.7  \n",
       "3       18.7  394.63   2.94  33.4  \n",
       "4       18.7  396.90   5.33  36.2  \n",
       "..       ...     ...    ...   ...  \n",
       "501     21.0  391.99   9.67  22.4  \n",
       "502     21.0  396.90   9.08  20.6  \n",
       "503     21.0  396.90   5.64  23.9  \n",
       "504     21.0  393.45   6.48  22.0  \n",
       "505     21.0  396.90   7.88  11.9  \n",
       "\n",
       "[506 rows x 14 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0086459",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=data.drop(columns=\"MEDV\") #independent Features\n",
    "y=data[\"MEDV\"] # Dependent/Target Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ec0f80",
   "metadata": {},
   "source": [
    "## Train test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "481d711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.33,random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2da5370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(339, 13)\n",
      "(339,)\n",
      "(167, 13)\n",
      "(167,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18b741a",
   "metadata": {},
   "source": [
    "## Model Building with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7421a374",
   "metadata": {},
   "source": [
    "- Model Init- DEfine the network architecture (#inputs,#outputs.#HLs,#Nodes) \n",
    "- MOdel compilation- Define the type of problem (Loss Function)\n",
    "- Model Train-Pass Dataser,#Iteration,TEst Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d0155f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "47384a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input,Dense\n",
    "from tensorflow.keras.optimizers import SGD,Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e35ffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential() #Epmty Model\n",
    "model.add(Input(13)) # INput LAyer\n",
    "model.add(Dense(5,activation=\"relu\")) #HL1\n",
    "model.add(Dense(1)) # Output LAyer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f5f586c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOdel Compilation\n",
    "# model.compile(optimizer=SGD(learning_rate=0.00001),loss=\"mean_squared_error\") #stochastic gradient Descent\n",
    "\n",
    "model.compile(optimizer=Adam(0.003),loss=\"mean_squared_error\") #stochastic gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de81c117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "11/11 [==============================] - 1s 25ms/step - loss: 33623.5547 - val_loss: 22914.4258\n",
      "Epoch 2/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 17617.5234 - val_loss: 12069.5654\n",
      "Epoch 3/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 9147.4922 - val_loss: 5959.5234\n",
      "Epoch 4/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4413.4697 - val_loss: 2486.6233\n",
      "Epoch 5/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1704.1252 - val_loss: 782.4908\n",
      "Epoch 6/250\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 558.9689 - val_loss: 413.5416\n",
      "Epoch 7/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 335.1602 - val_loss: 438.7753\n",
      "Epoch 8/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 305.5521 - val_loss: 355.7452\n",
      "Epoch 9/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 248.9487 - val_loss: 279.5432\n",
      "Epoch 10/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 216.4001 - val_loss: 245.9914\n",
      "Epoch 11/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 200.6180 - val_loss: 221.5410\n",
      "Epoch 12/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 183.5816 - val_loss: 207.2980\n",
      "Epoch 13/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 169.1123 - val_loss: 194.2233\n",
      "Epoch 14/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 158.4068 - val_loss: 179.2819\n",
      "Epoch 15/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 148.8776 - val_loss: 167.4654\n",
      "Epoch 16/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 140.8848 - val_loss: 157.8280\n",
      "Epoch 17/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 134.5590 - val_loss: 148.9233\n",
      "Epoch 18/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 127.7237 - val_loss: 142.6511\n",
      "Epoch 19/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 122.8587 - val_loss: 136.7287\n",
      "Epoch 20/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 117.8449 - val_loss: 129.4891\n",
      "Epoch 21/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 113.7767 - val_loss: 123.1828\n",
      "Epoch 22/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 109.9352 - val_loss: 118.5832\n",
      "Epoch 23/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 106.5970 - val_loss: 114.5393\n",
      "Epoch 24/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 103.9807 - val_loss: 110.8412\n",
      "Epoch 25/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 100.8724 - val_loss: 105.9798\n",
      "Epoch 26/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 97.9404 - val_loss: 103.5936\n",
      "Epoch 27/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 95.7185 - val_loss: 100.4299\n",
      "Epoch 28/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 93.5302 - val_loss: 96.4196\n",
      "Epoch 29/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 91.6181 - val_loss: 93.5678\n",
      "Epoch 30/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 89.2394 - val_loss: 91.8967\n",
      "Epoch 31/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 87.8544 - val_loss: 90.0615\n",
      "Epoch 32/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 86.0128 - val_loss: 86.1622\n",
      "Epoch 33/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 84.5810 - val_loss: 84.0082\n",
      "Epoch 34/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 83.6491 - val_loss: 83.6474\n",
      "Epoch 35/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 81.4380 - val_loss: 80.1773\n",
      "Epoch 36/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 80.3300 - val_loss: 78.6586\n",
      "Epoch 37/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 79.0248 - val_loss: 77.0784\n",
      "Epoch 38/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 77.8489 - val_loss: 75.8735\n",
      "Epoch 39/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 76.7161 - val_loss: 73.8733\n",
      "Epoch 40/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 75.9796 - val_loss: 72.4593\n",
      "Epoch 41/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 75.1406 - val_loss: 72.0552\n",
      "Epoch 42/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 74.4002 - val_loss: 70.5680\n",
      "Epoch 43/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 73.1685 - val_loss: 68.5746\n",
      "Epoch 44/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 72.5873 - val_loss: 67.8690\n",
      "Epoch 45/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 71.9475 - val_loss: 67.4096\n",
      "Epoch 46/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 71.1697 - val_loss: 65.8629\n",
      "Epoch 47/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 71.0729 - val_loss: 65.7679\n",
      "Epoch 48/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 70.1780 - val_loss: 64.1324\n",
      "Epoch 49/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 69.5723 - val_loss: 63.4410\n",
      "Epoch 50/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 69.1141 - val_loss: 62.7642\n",
      "Epoch 51/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 68.7721 - val_loss: 61.8896\n",
      "Epoch 52/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 68.1457 - val_loss: 61.9609\n",
      "Epoch 53/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 67.7390 - val_loss: 60.8149\n",
      "Epoch 54/250\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 67.1754 - val_loss: 60.5661\n",
      "Epoch 55/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 66.9476 - val_loss: 60.2188\n",
      "Epoch 56/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 66.6215 - val_loss: 59.2076\n",
      "Epoch 57/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 66.5833 - val_loss: 59.6388\n",
      "Epoch 58/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 65.9514 - val_loss: 58.2340\n",
      "Epoch 59/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 65.7976 - val_loss: 57.7764\n",
      "Epoch 60/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 65.6812 - val_loss: 57.8543\n",
      "Epoch 61/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 64.8509 - val_loss: 56.8677\n",
      "Epoch 62/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 64.7691 - val_loss: 56.6314\n",
      "Epoch 63/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 64.3205 - val_loss: 56.2113\n",
      "Epoch 64/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 64.3531 - val_loss: 56.0233\n",
      "Epoch 65/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 63.9271 - val_loss: 55.4919\n",
      "Epoch 66/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 63.8890 - val_loss: 55.3349\n",
      "Epoch 67/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 63.6762 - val_loss: 54.9959\n",
      "Epoch 68/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 63.2091 - val_loss: 54.9447\n",
      "Epoch 69/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 63.0643 - val_loss: 54.5837\n",
      "Epoch 70/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 62.9429 - val_loss: 54.2088\n",
      "Epoch 71/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 62.6086 - val_loss: 53.9285\n",
      "Epoch 72/250\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 62.5771 - val_loss: 53.7697\n",
      "Epoch 73/250\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 62.6992 - val_loss: 53.4904\n",
      "Epoch 74/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 62.5309 - val_loss: 53.6710\n",
      "Epoch 75/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 62.2127 - val_loss: 53.1663\n",
      "Epoch 76/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 61.8613 - val_loss: 53.1606\n",
      "Epoch 77/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 61.6624 - val_loss: 52.9055\n",
      "Epoch 78/250\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 61.5296 - val_loss: 52.5210\n",
      "Epoch 79/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 61.7099 - val_loss: 52.5328\n",
      "Epoch 80/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 61.5504 - val_loss: 52.2091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 62.0658 - val_loss: 52.1703\n",
      "Epoch 82/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 60.9349 - val_loss: 51.8667\n",
      "Epoch 83/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 60.8913 - val_loss: 51.7239\n",
      "Epoch 84/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 60.6689 - val_loss: 51.5506\n",
      "Epoch 85/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 60.8948 - val_loss: 51.4324\n",
      "Epoch 86/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 60.5888 - val_loss: 51.4352\n",
      "Epoch 87/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 60.6037 - val_loss: 51.3659\n",
      "Epoch 88/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 60.8200 - val_loss: 51.1052\n",
      "Epoch 89/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 60.0075 - val_loss: 51.1212\n",
      "Epoch 90/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 60.3622 - val_loss: 50.6810\n",
      "Epoch 91/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 60.3284 - val_loss: 50.7412\n",
      "Epoch 92/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 59.5244 - val_loss: 50.4234\n",
      "Epoch 93/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 59.8269 - val_loss: 50.2007\n",
      "Epoch 94/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 60.1698 - val_loss: 50.5677\n",
      "Epoch 95/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 59.2605 - val_loss: 49.9388\n",
      "Epoch 96/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 59.8639 - val_loss: 49.7920\n",
      "Epoch 97/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 60.0063 - val_loss: 50.1743\n",
      "Epoch 98/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 59.2000 - val_loss: 50.1752\n",
      "Epoch 99/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 59.5996 - val_loss: 49.8098\n",
      "Epoch 100/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 59.4554 - val_loss: 49.3134\n",
      "Epoch 101/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 59.1027 - val_loss: 49.1651\n",
      "Epoch 102/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 58.7081 - val_loss: 49.0979\n",
      "Epoch 103/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 58.7260 - val_loss: 48.9915\n",
      "Epoch 104/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 58.3851 - val_loss: 49.0243\n",
      "Epoch 105/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 58.4599 - val_loss: 48.7752\n",
      "Epoch 106/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 58.3079 - val_loss: 48.7628\n",
      "Epoch 107/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 58.3316 - val_loss: 48.7104\n",
      "Epoch 108/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 58.3421 - val_loss: 48.5389\n",
      "Epoch 109/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 58.3965 - val_loss: 48.7499\n",
      "Epoch 110/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 57.8970 - val_loss: 48.3162\n",
      "Epoch 111/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 58.8510 - val_loss: 48.1482\n",
      "Epoch 112/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 58.8949 - val_loss: 48.5317\n",
      "Epoch 113/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 57.9542 - val_loss: 47.9657\n",
      "Epoch 114/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 58.0049 - val_loss: 47.9132\n",
      "Epoch 115/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 57.9450 - val_loss: 48.2500\n",
      "Epoch 116/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 57.5334 - val_loss: 47.7852\n",
      "Epoch 117/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 57.4317 - val_loss: 47.7356\n",
      "Epoch 118/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 57.3930 - val_loss: 47.5504\n",
      "Epoch 119/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 57.1760 - val_loss: 47.4578\n",
      "Epoch 120/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 57.1460 - val_loss: 47.4437\n",
      "Epoch 121/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 57.1672 - val_loss: 47.3208\n",
      "Epoch 122/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 57.0275 - val_loss: 47.2728\n",
      "Epoch 123/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 56.9586 - val_loss: 47.2623\n",
      "Epoch 124/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 56.7756 - val_loss: 47.0680\n",
      "Epoch 125/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 56.8392 - val_loss: 47.0104\n",
      "Epoch 126/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 56.7068 - val_loss: 46.9313\n",
      "Epoch 127/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 56.7452 - val_loss: 46.7939\n",
      "Epoch 128/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 56.9168 - val_loss: 46.7528\n",
      "Epoch 129/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 56.3466 - val_loss: 46.8093\n",
      "Epoch 130/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 56.8929 - val_loss: 46.6991\n",
      "Epoch 131/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 56.1830 - val_loss: 46.6359\n",
      "Epoch 132/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 56.8122 - val_loss: 46.5508\n",
      "Epoch 133/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 56.3074 - val_loss: 46.4055\n",
      "Epoch 134/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 56.3027 - val_loss: 46.2264\n",
      "Epoch 135/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 56.5674 - val_loss: 46.2180\n",
      "Epoch 136/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 55.8671 - val_loss: 46.0286\n",
      "Epoch 137/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 55.8088 - val_loss: 46.0616\n",
      "Epoch 138/250\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 55.7459 - val_loss: 45.9482\n",
      "Epoch 139/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 55.5438 - val_loss: 45.9469\n",
      "Epoch 140/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 55.6229 - val_loss: 45.8492\n",
      "Epoch 141/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 55.9260 - val_loss: 45.7186\n",
      "Epoch 142/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 55.3302 - val_loss: 45.7198\n",
      "Epoch 143/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 55.3379 - val_loss: 45.4777\n",
      "Epoch 144/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 56.1072 - val_loss: 45.4126\n",
      "Epoch 145/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 55.3459 - val_loss: 45.5344\n",
      "Epoch 146/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 55.1743 - val_loss: 45.3298\n",
      "Epoch 147/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 55.4809 - val_loss: 45.2078\n",
      "Epoch 148/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 56.8453 - val_loss: 45.2681\n",
      "Epoch 149/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 55.4111 - val_loss: 45.7142\n",
      "Epoch 150/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 56.3147 - val_loss: 45.4860\n",
      "Epoch 151/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 55.5846 - val_loss: 45.7578\n",
      "Epoch 152/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 55.1639 - val_loss: 45.3743\n",
      "Epoch 153/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 54.5775 - val_loss: 44.8831\n",
      "Epoch 154/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 54.6906 - val_loss: 44.7984\n",
      "Epoch 155/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 54.5279 - val_loss: 44.7493\n",
      "Epoch 156/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 54.9974 - val_loss: 44.7511\n",
      "Epoch 157/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 56.4406 - val_loss: 44.8307\n",
      "Epoch 158/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 55.9589 - val_loss: 45.0213\n",
      "Epoch 159/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 54.2504 - val_loss: 44.7096\n",
      "Epoch 160/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 5ms/step - loss: 54.8103 - val_loss: 44.6270\n",
      "Epoch 161/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 54.6112 - val_loss: 44.4933\n",
      "Epoch 162/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 54.3457 - val_loss: 44.4320\n",
      "Epoch 163/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 54.2971 - val_loss: 44.7181\n",
      "Epoch 164/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 53.6954 - val_loss: 44.4835\n",
      "Epoch 165/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 54.0323 - val_loss: 44.3043\n",
      "Epoch 166/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 53.6981 - val_loss: 44.2930\n",
      "Epoch 167/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 53.9086 - val_loss: 44.1451\n",
      "Epoch 168/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 55.1092 - val_loss: 44.0862\n",
      "Epoch 169/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 53.8037 - val_loss: 44.3772\n",
      "Epoch 170/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 53.5939 - val_loss: 44.1275\n",
      "Epoch 171/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 53.5971 - val_loss: 43.8482\n",
      "Epoch 172/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 53.4822 - val_loss: 43.7786\n",
      "Epoch 173/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 53.3768 - val_loss: 43.7702\n",
      "Epoch 174/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 53.3465 - val_loss: 43.7671\n",
      "Epoch 175/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 53.1882 - val_loss: 43.7282\n",
      "Epoch 176/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 53.2128 - val_loss: 43.6331\n",
      "Epoch 177/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 53.1296 - val_loss: 43.5489\n",
      "Epoch 178/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 53.1853 - val_loss: 43.4735\n",
      "Epoch 179/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 53.2377 - val_loss: 43.4099\n",
      "Epoch 180/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 52.9847 - val_loss: 43.3248\n",
      "Epoch 181/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 52.9305 - val_loss: 43.3689\n",
      "Epoch 182/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 53.0957 - val_loss: 43.2443\n",
      "Epoch 183/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 53.1451 - val_loss: 43.2900\n",
      "Epoch 184/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 52.9115 - val_loss: 43.1826\n",
      "Epoch 185/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 52.9363 - val_loss: 43.1049\n",
      "Epoch 186/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 52.7751 - val_loss: 43.0864\n",
      "Epoch 187/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 52.7333 - val_loss: 43.0276\n",
      "Epoch 188/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 52.4772 - val_loss: 42.9520\n",
      "Epoch 189/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 52.5515 - val_loss: 42.9167\n",
      "Epoch 190/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 52.4998 - val_loss: 42.8562\n",
      "Epoch 191/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 52.5770 - val_loss: 42.8211\n",
      "Epoch 192/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 52.3807 - val_loss: 42.7937\n",
      "Epoch 193/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 53.4775 - val_loss: 42.7852\n",
      "Epoch 194/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 53.3639 - val_loss: 42.8765\n",
      "Epoch 195/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 52.7338 - val_loss: 42.5813\n",
      "Epoch 196/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 52.1248 - val_loss: 42.5278\n",
      "Epoch 197/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 51.9710 - val_loss: 42.3683\n",
      "Epoch 198/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 52.0914 - val_loss: 42.3711\n",
      "Epoch 199/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 52.3843 - val_loss: 42.3876\n",
      "Epoch 200/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 52.6238 - val_loss: 42.3032\n",
      "Epoch 201/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 52.1856 - val_loss: 42.4397\n",
      "Epoch 202/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 52.9657 - val_loss: 42.2789\n",
      "Epoch 203/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 51.8523 - val_loss: 42.1516\n",
      "Epoch 204/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 51.8482 - val_loss: 42.1557\n",
      "Epoch 205/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 51.6735 - val_loss: 42.0451\n",
      "Epoch 206/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 51.5394 - val_loss: 42.0480\n",
      "Epoch 207/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 51.2019 - val_loss: 42.0603\n",
      "Epoch 208/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 51.8124 - val_loss: 41.9474\n",
      "Epoch 209/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 51.7503 - val_loss: 41.9028\n",
      "Epoch 210/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 51.4178 - val_loss: 41.7160\n",
      "Epoch 211/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 51.9179 - val_loss: 41.6902\n",
      "Epoch 212/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 51.6736 - val_loss: 41.5867\n",
      "Epoch 213/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 51.1351 - val_loss: 41.7088\n",
      "Epoch 214/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 50.9847 - val_loss: 41.7534\n",
      "Epoch 215/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 51.1188 - val_loss: 41.6465\n",
      "Epoch 216/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 50.9947 - val_loss: 41.4694\n",
      "Epoch 217/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 50.8424 - val_loss: 41.4483\n",
      "Epoch 218/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 50.8605 - val_loss: 41.3509\n",
      "Epoch 219/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 50.9886 - val_loss: 41.2671\n",
      "Epoch 220/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 51.2293 - val_loss: 41.2644\n",
      "Epoch 221/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 50.5175 - val_loss: 41.2651\n",
      "Epoch 222/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 50.6122 - val_loss: 41.2336\n",
      "Epoch 223/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 50.7862 - val_loss: 41.2905\n",
      "Epoch 224/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 50.5277 - val_loss: 41.2483\n",
      "Epoch 225/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 50.4281 - val_loss: 41.0806\n",
      "Epoch 226/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 50.1964 - val_loss: 41.0151\n",
      "Epoch 227/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 50.1336 - val_loss: 40.9669\n",
      "Epoch 228/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 50.3903 - val_loss: 41.0045\n",
      "Epoch 229/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 50.1318 - val_loss: 41.0735\n",
      "Epoch 230/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 50.3243 - val_loss: 40.9209\n",
      "Epoch 231/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 50.0273 - val_loss: 40.8594\n",
      "Epoch 232/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 50.3617 - val_loss: 40.6335\n",
      "Epoch 233/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 49.7785 - val_loss: 40.6484\n",
      "Epoch 234/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 49.9109 - val_loss: 40.8505\n",
      "Epoch 235/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 49.7126 - val_loss: 40.5318\n",
      "Epoch 236/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 49.6284 - val_loss: 40.4116\n",
      "Epoch 237/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 49.5751 - val_loss: 40.4060\n",
      "Epoch 238/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 49.4206 - val_loss: 40.4578\n",
      "Epoch 239/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 5ms/step - loss: 49.9559 - val_loss: 40.3404\n",
      "Epoch 240/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 49.4825 - val_loss: 40.5365\n",
      "Epoch 241/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 49.3703 - val_loss: 40.2704\n",
      "Epoch 242/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 49.5954 - val_loss: 40.2669\n",
      "Epoch 243/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 49.7363 - val_loss: 40.1534\n",
      "Epoch 244/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 49.1673 - val_loss: 40.0945\n",
      "Epoch 245/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 49.4237 - val_loss: 40.0401\n",
      "Epoch 246/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 49.3083 - val_loss: 39.9231\n",
      "Epoch 247/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 49.0106 - val_loss: 40.0538\n",
      "Epoch 248/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 48.6496 - val_loss: 40.2283\n",
      "Epoch 249/250\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 49.5875 - val_loss: 40.0227\n",
      "Epoch 250/250\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 49.2028 - val_loss: 39.8289\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c447b3ffa0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "model.fit(x=x_train,y=y_train,epochs=250,validation_data=(x_test,y_test),batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8be54d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f3980616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'val_loss'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c5fd51fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee115455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_12956\\2217070938.py:7: UserWarning: Legend does not support 'T' instances.\n",
      "A proxy artist may be used instead.\n",
      "See: https://matplotlib.org/users/legend_guide.html#creating-artists-specifically-for-adding-to-the-legend-aka-proxy-artists\n",
      "  plt.legend(\"Train\",\"Validation\")\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_12956\\2217070938.py:7: UserWarning: Legend does not support 'r' instances.\n",
      "A proxy artist may be used instead.\n",
      "See: https://matplotlib.org/users/legend_guide.html#creating-artists-specifically-for-adding-to-the-legend-aka-proxy-artists\n",
      "  plt.legend(\"Train\",\"Validation\")\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_12956\\2217070938.py:7: UserWarning: Legend does not support 'a' instances.\n",
      "A proxy artist may be used instead.\n",
      "See: https://matplotlib.org/users/legend_guide.html#creating-artists-specifically-for-adding-to-the-legend-aka-proxy-artists\n",
      "  plt.legend(\"Train\",\"Validation\")\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_12956\\2217070938.py:7: UserWarning: Legend does not support 'i' instances.\n",
      "A proxy artist may be used instead.\n",
      "See: https://matplotlib.org/users/legend_guide.html#creating-artists-specifically-for-adding-to-the-legend-aka-proxy-artists\n",
      "  plt.legend(\"Train\",\"Validation\")\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_12956\\2217070938.py:7: UserWarning: Legend does not support 'n' instances.\n",
      "A proxy artist may be used instead.\n",
      "See: https://matplotlib.org/users/legend_guide.html#creating-artists-specifically-for-adding-to-the-legend-aka-proxy-artists\n",
      "  plt.legend(\"Train\",\"Validation\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss=history[\"loss\"]\n",
    "val_loss=history[\"val_loss\"]\n",
    "\n",
    "plt.plot(train_loss)\n",
    "plt.plot(val_loss)\n",
    "plt.ylim(0,200)\n",
    "plt.legend(\"Train\",\"Validation\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.title(\"Loss Curve (MSE)\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7bbb7c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_4/kernel:0' shape=(13, 5) dtype=float32, numpy=\n",
       " array([[-0.1076422 , -0.19538507,  0.05040693,  0.07185936, -0.3489045 ],\n",
       "        [ 0.17641625, -0.02412497, -0.18553679,  0.22177856, -0.37469986],\n",
       "        [-0.05530841, -0.4044504 ,  0.54581314,  0.44049338, -0.16692403],\n",
       "        [-0.32567763,  0.841289  , -0.9406552 ,  0.3219539 , -0.13198972],\n",
       "        [ 0.4782267 ,  0.38717154, -0.01910173,  0.2413412 , -0.32687137],\n",
       "        [-0.4313871 , -0.09770198, -0.81755817, -0.23233499,  0.2862708 ],\n",
       "        [ 0.11625116,  0.25987858,  0.60929567, -0.24580038, -0.36781994],\n",
       "        [-0.29880464, -0.07791305,  0.1414012 ,  0.42876065,  0.30528134],\n",
       "        [-0.06625547, -0.29241154,  0.45635563,  0.43034035,  0.00989282],\n",
       "        [-0.20126179,  0.1407584 ,  0.33756247, -0.18674433, -0.5726967 ],\n",
       "        [-0.33057955,  0.34794196,  0.4932724 , -0.24552207,  0.40004814],\n",
       "        [ 0.14871718,  0.05834627, -0.02026282,  0.06061609,  0.06475735],\n",
       "        [ 0.0406166 , -0.9022365 ,  0.46705967, -0.5122633 ,  0.28940815]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_4/bias:0' shape=(5,) dtype=float32, numpy=\n",
       " array([-0.11571117,  0.08275398, -0.33612233, -0.07056223,  0.        ],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_5/kernel:0' shape=(5, 1) dtype=float32, numpy=\n",
       " array([[ 0.64323896],\n",
       "        [ 0.61695534],\n",
       "        [-0.15799917],\n",
       "        [ 0.9292583 ],\n",
       "        [-0.5284612 ]], dtype=float32)>,\n",
       " <tf.Variable 'dense_5/bias:0' shape=(1,) dtype=float32, numpy=array([0.11150644], dtype=float32)>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2f0b8896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "43944f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>15.0</td>\n",
       "      <td>7.160636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>26.6</td>\n",
       "      <td>26.919146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>45.4</td>\n",
       "      <td>34.418095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>20.8</td>\n",
       "      <td>20.158995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>34.9</td>\n",
       "      <td>29.828112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>42.3</td>\n",
       "      <td>28.613836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>17.9</td>\n",
       "      <td>-3.040795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>12.7</td>\n",
       "      <td>7.015777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>50.0</td>\n",
       "      <td>40.070332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>18.4</td>\n",
       "      <td>24.351919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>167 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     actual      preds\n",
       "410    15.0   7.160636\n",
       "85     26.6  26.919146\n",
       "280    45.4  34.418095\n",
       "422    20.8  20.158995\n",
       "199    34.9  29.828112\n",
       "..      ...        ...\n",
       "202    42.3  28.613836\n",
       "412    17.9  -3.040795\n",
       "456    12.7   7.015777\n",
       "204    50.0  40.070332\n",
       "442    18.4  24.351919\n",
       "\n",
       "[167 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"actual\":y_test,\"preds\":y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0ef93279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error,mean_squared_error,r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b1485cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5371907180699698"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "544e9133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25500565469045156"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_percentage_error(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c72e13a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.82885966920212"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b1490221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 5)                 70        \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 76\n",
      "Trainable params: 76\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "189b14f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRIM       0\n",
       "ZN         0\n",
       "INDUS      0\n",
       "CHAS       0\n",
       "NOX        0\n",
       "RM         0\n",
       "AGE        0\n",
       "DIS        0\n",
       "RAD        0\n",
       "TAX        0\n",
       "PTRATIO    0\n",
       "B          0\n",
       "LSTAT      0\n",
       "MEDV       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "96a80532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 14 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   CRIM     506 non-null    float64\n",
      " 1   ZN       506 non-null    float64\n",
      " 2   INDUS    506 non-null    float64\n",
      " 3   CHAS     506 non-null    float64\n",
      " 4   NOX      506 non-null    float64\n",
      " 5   RM       506 non-null    float64\n",
      " 6   AGE      506 non-null    float64\n",
      " 7   DIS      506 non-null    float64\n",
      " 8   RAD      506 non-null    float64\n",
      " 9   TAX      506 non-null    float64\n",
      " 10  PTRATIO  506 non-null    float64\n",
      " 11  B        506 non-null    float64\n",
      " 12  LSTAT    506 non-null    float64\n",
      " 13  MEDV     506 non-null    float64\n",
      "dtypes: float64(14)\n",
      "memory usage: 55.5 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a26e41b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
